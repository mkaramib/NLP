{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization_Transformers_Decoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdTus54yB86JlmlGnD8ce9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/TextSummarization_Transformers_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZzcJSBmzUn"
      },
      "source": [
        "# Text Summarization using Transformers based on GPT (Decoder)\r\n",
        "This jupyter notebook implements a text summarization using Transformerts. It employes Trax to train a summarizer on cnn daily news. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2MCb29nt3m"
      },
      "source": [
        "## Install Libraries\r\n",
        "this section, required libraries such as Trax and SentencePiece will be installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RYs5FgGn79W"
      },
      "source": [
        "# install SentencePiece\r\n",
        "!pip install sentencepiece\r\n",
        "\r\n",
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "\r\n",
        "# check the Trax version\r\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTCBHsmofO0"
      },
      "source": [
        "## Import Libraries\r\n",
        "Required libraries will be imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWyOdC4msvX"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import gin\r\n",
        "import textwrap\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.fastmath import numpy as jnp\r\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4302NMiknjh5"
      },
      "source": [
        "## Initializations\r\n",
        "This section will initialize required parameters of libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSScCxRcnrrJ"
      },
      "source": [
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJFEUvmqnnK"
      },
      "source": [
        "# Data Prepare\r\n",
        "In this section, the data will be prepared and converted to tensor in the required format for the Transformers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsjEvs-rKpY"
      },
      "source": [
        "# read data\r\n",
        "\r\n",
        "# train the sentencepiece for tokenization and vocabulary\r\n",
        "\r\n",
        "# tokenize them using sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2JZWmJSreZe"
      },
      "source": [
        "# Transformer\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tS8jpoNrg1Q"
      },
      "source": [
        "## Embedding and Positional Encoding\r\n",
        "In this section, an Embedding and Positional Encoding will be implemented. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqk3FcanF1W"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JHSv8fsrrAJ"
      },
      "source": [
        "## Feedforward Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXFhoS0rxDN"
      },
      "source": [
        "## Decoder Layer"
      ]
    }
  ]
}