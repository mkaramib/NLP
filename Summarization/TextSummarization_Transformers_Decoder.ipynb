{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization_Transformers_Decoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMc+f2YfB0ngqwuhnJkIEFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/Summarization/TextSummarization_Transformers_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZzcJSBmzUn"
      },
      "source": [
        "# Text Summarization using Transformers based on GPT (Decoder)\r\n",
        "This jupyter notebook implements a text summarization using Transformerts. It employes Trax to train a summarizer on cnn daily news. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2MCb29nt3m"
      },
      "source": [
        "## Install Libraries\r\n",
        "this section, required libraries such as Trax and SentencePiece will be installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RYs5FgGn79W"
      },
      "source": [
        "# install SentencePiece\r\n",
        "!pip install sentencepiece\r\n",
        "\r\n",
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "\r\n",
        "# check the Trax version\r\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTCBHsmofO0"
      },
      "source": [
        "## Import Libraries\r\n",
        "Required libraries will be imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWyOdC4msvX"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import gin\r\n",
        "import textwrap\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.fastmath import numpy as jnp\r\n",
        "import sentencepiece as spm\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4302NMiknjh5"
      },
      "source": [
        "## Initializations\r\n",
        "This section will initialize required parameters of libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSScCxRcnrrJ"
      },
      "source": [
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJFEUvmqnnK"
      },
      "source": [
        "# Data Prepare\r\n",
        "In this section, the data will be prepared and converted to tensor in the required format for the Transformers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmvKY1tUR1y2"
      },
      "source": [
        "### Train SentencePiece and Build Vocabulary\r\n",
        "In this section, a vocabulary will be built using sentencepiece. It will contain sub-words. Generated model will be saved in *./data/mbpe.model*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBwWgSFRSBSs"
      },
      "source": [
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\r\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\r\n",
        "spm.SentencePieceTrainer.train('--input=./data/vocab_train.txt --model_prefix=./data/mbpe --model_type=bpe --vocab_size=20000')\r\n",
        "\r\n",
        "# changind the pre-defined symbols\r\n",
        "#spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\r\n",
        "\r\n",
        "# makes segmenter instance and loads the model file (m.model)\r\n",
        "sp = spm.SentencePieceProcessor()\r\n",
        "sp.load('./data/mbpe.model')\r\n",
        "\r\n",
        "# get the size of vocabulary\r\n",
        "vocab_size = sp.get_piece_size()\r\n",
        "\r\n",
        "# get the vocabulary\r\n",
        "vocab = [sp.id_to_piece(id) for id in range(vocab_size)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma-UL4tUsk74"
      },
      "source": [
        "# print vocab size\r\n",
        "print(f'vocab size = {vocab_size}')\r\n",
        "\r\n",
        "# get some vocabs \r\n",
        "print(vocab[0:20], sep=\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD5xcd3pVtYd"
      },
      "source": [
        "### Test tokenization\r\n",
        "Here are an example to see whether the tokenization using sentencepiece is working or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMy3VRlZVVf1"
      },
      "source": [
        "# encode: text => id\r\n",
        "s1 = \"Four groups that advocate for immigrant rights said Thursday they will challenge Arizona 's new immigration law , which allows police to ask anyone for proof of legal U.S. residency .\"\r\n",
        "\r\n",
        "# print the encoded results, tokens and ids\r\n",
        "print(sp.encode_as_pieces(s1))\r\n",
        "print(sp.encode_as_ids(s1))\r\n",
        "\r\n",
        "# print decoded \r\n",
        "print(sp.decode_pieces(sp.encode_as_pieces(s1)))\r\n",
        "print(sp.decode_ids([1758, 1093, 32, 21, 3370, 25, 8133, 485, 26, 222, 70, 58, 959]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt-_2WOPqFAM"
      },
      "source": [
        "### Trax tokenizer\r\n",
        "This section will see how the trax tokenizer works\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF-UJtAkqLmg"
      },
      "source": [
        "def tokenize(input_str, EOS=1):\r\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\r\n",
        "  \r\n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\r\n",
        "    # we get around it by making a 1-element stream with `iter`.\r\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\r\n",
        "                                      vocab_dir='./data/',\r\n",
        "                                      vocab_file='mbpe.vocab'))\r\n",
        "    \r\n",
        "    # Mark the end of the sentence with EOS\r\n",
        "    return list(inputs) + [EOS]\r\n",
        "\r\n",
        "def detokenize(integers):\r\n",
        "    \"\"\"List of ints to str\"\"\"\r\n",
        "  \r\n",
        "    s = trax.data.detokenize(integers,\r\n",
        "                             vocab_dir='./data/',\r\n",
        "                             vocab_file='mbpe.vocab')\r\n",
        "    \r\n",
        "    return wrapper.fill(s)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy_WAIeeV993"
      },
      "source": [
        "### Data Load\r\n",
        "In this section, the data will be loaded. Data is stored in csv file using two columns of *body* and *highlights*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsjEvs-rKpY"
      },
      "source": [
        "# data file\r\n",
        "cnn_file = \"./data/cnn_1000.csv\"\r\n",
        "\r\n",
        "# read data\r\n",
        "cnn = pd.read_csv(cnn_file, sep=\"\\t\")\r\n",
        "cnn_articles = cnn[\"body\"]\r\n",
        "cnn_highlights = cnn[\"highlights\"]\r\n",
        "\r\n",
        "# train the sentencepiece for tokenization and vocabulary\r\n",
        "\r\n",
        "# tokenize them using sentencepiece"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV03CZ2smlOA"
      },
      "source": [
        "# test data loading\r\n",
        "print(f'num of sampels = {len(cnn_articles)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2JZWmJSreZe"
      },
      "source": [
        "# Transformer - Decoder Only\r\n",
        "This experiment is based on using only the Decoder part of Transformer. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tS8jpoNrg1Q"
      },
      "source": [
        "## Embedding and Positional Encoding\r\n",
        "In this section, an Embedding and Positional Encoding will be implemented. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqk3FcanF1W"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JHSv8fsrrAJ"
      },
      "source": [
        "## Feedforward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23W5eUxkmxGQ"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a feed-forward block.\r\n",
        "    The input is an activation tensor.\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\r\n",
        "        ff_activation(),  # Generally ReLU\r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXFhoS0rxDN"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCPmTpynLzO"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a Transformer decoder block.\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode)\r\n",
        "          # Add dropout with rate and mode specified\r\n",
        "          tl.Dropout(rate=dropout, mode=mode)\r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypWsN6XopLhx"
      },
      "source": [
        "## Transformer LM\r\n",
        "In this section, a decoder-based Transformer is implemented.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOS0JifkpUPu"
      },
      "source": [
        "# A Transformer using only the Docoder part\r\n",
        "def TransformerLM(vocab_size=33300,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"\r\n",
        "    Returns a Transformer language model.\r\n",
        "    The input to the model is a tensor of tokens. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    positional_encoder = [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),\r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout,mode=mode),\r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\r\n",
        "\r\n",
        "    # Create the complete model as written in the figure\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), # Specify the mode!\r\n",
        "        # Add positional encoder\r\n",
        "        positional_encoder,\r\n",
        "        # Add decoder blocks,(list of Decoders)\r\n",
        "        [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)],\r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(),\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size),\r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax()\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5I-P44L1hQP"
      },
      "source": [
        "# Training\r\n",
        "In this section, implemented Decoder-based Transformer will be trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hv5zf0u2jpI"
      },
      "source": [
        "def training_loop(TransformerLM, \r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\r\n",
        "\r\n",
        "    # define the training task\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data= train_gen, # The training generator\r\n",
        "      loss_layer= tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer= trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\r\n",
        "      lr_schedule= lr_schedule,\r\n",
        "      n_steps_per_checkpoint=10\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the evaluation task\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data= eval_gen, # The evaluation generator\r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the training loop\r\n",
        "    loop = training.Loop(TransformerLM(d_model=4,\r\n",
        "                                       d_ff=16,\r\n",
        "                                       n_layers=1,\r\n",
        "                                       n_heads=2,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}