{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization_Transformers_Decoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOGx6YgH5IiuaR5wVGwW/kU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/Summarization/TextSummarization_Transformers_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZzcJSBmzUn"
      },
      "source": [
        "# Text Summarization using Transformers based on GPT (Decoder)\r\n",
        "This jupyter notebook implements a text summarization using Transformerts. It employes Trax to train a summarizer on cnn daily news. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2MCb29nt3m"
      },
      "source": [
        "## Install Libraries\r\n",
        "this section, required libraries such as Trax and SentencePiece will be installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RYs5FgGn79W"
      },
      "source": [
        "# install SentencePiece\r\n",
        "!pip install sentencepiece\r\n",
        "\r\n",
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "\r\n",
        "# check the Trax version\r\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTCBHsmofO0"
      },
      "source": [
        "## Import Libraries\r\n",
        "Required libraries will be imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWyOdC4msvX"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import gin\r\n",
        "import textwrap\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.fastmath import numpy as jnp\r\n",
        "import sentencepiece as spm\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4302NMiknjh5"
      },
      "source": [
        "## Initializations\r\n",
        "This section will initialize required parameters of libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSScCxRcnrrJ"
      },
      "source": [
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJFEUvmqnnK"
      },
      "source": [
        "# Data Prepare\r\n",
        "In this section, the data will be prepared and converted to tensor in the required format for the Transformers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsjEvs-rKpY"
      },
      "source": [
        "# read data\r\n",
        "\r\n",
        "# train the sentencepiece for tokenization and vocabulary\r\n",
        "\r\n",
        "# tokenize them using sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2JZWmJSreZe"
      },
      "source": [
        "# Transformer - Decoder Only\r\n",
        "This experiment is based on using only the Decoder part of Transformer. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tS8jpoNrg1Q"
      },
      "source": [
        "## Embedding and Positional Encoding\r\n",
        "In this section, an Embedding and Positional Encoding will be implemented. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqk3FcanF1W"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JHSv8fsrrAJ"
      },
      "source": [
        "## Feedforward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23W5eUxkmxGQ"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a feed-forward block.\r\n",
        "    The input is an activation tensor.\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\r\n",
        "        ff_activation(),  # Generally ReLU\r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXFhoS0rxDN"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCPmTpynLzO"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a Transformer decoder block.\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode)\r\n",
        "          # Add dropout with rate and mode specified\r\n",
        "          tl.Dropout(rate=dropout, mode=mode)\r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypWsN6XopLhx"
      },
      "source": [
        "## Transformer LM\r\n",
        "In this section, a decoder-based Transformer is implemented.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOS0JifkpUPu"
      },
      "source": [
        "# A Transformer using only the Docoder part\r\n",
        "def TransformerLM(vocab_size=33300,\r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"\r\n",
        "    Returns a Transformer language model.\r\n",
        "    The input to the model is a tensor of tokens. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\r\n",
        "    \r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    positional_encoder = [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),\r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout,mode=mode),\r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\r\n",
        "\r\n",
        "    # Create the complete model as written in the figure\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), # Specify the mode!\r\n",
        "        # Add positional encoder\r\n",
        "        positional_encoder,\r\n",
        "        # Add decoder blocks,(list of Decoders)\r\n",
        "        [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)],\r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(),\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size),\r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax()\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5I-P44L1hQP"
      },
      "source": [
        "# Training\r\n",
        "In this section, implemented Decoder-based Transformer will be trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hv5zf0u2jpI"
      },
      "source": [
        "def training_loop(TransformerLM, \r\n",
        "                  d_model=512,\r\n",
        "                  d_ff=2048,\r\n",
        "                  n_layers=6,\r\n",
        "                  n_heads=8,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  train_gen, eval_gen, output_dir = \"~/model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\r\n",
        "\r\n",
        "    # define the training task\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data= train_gen, # The training generator\r\n",
        "      loss_layer= tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer= trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\r\n",
        "      lr_schedule= lr_schedule,\r\n",
        "      n_steps_per_checkpoint=10\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the evaluation task\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data= eval_gen, # The evaluation generator\r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the training loop\r\n",
        "    loop = training.Loop(TransformerLM(d_model=4,\r\n",
        "                                       d_ff=16,\r\n",
        "                                       n_layers=1,\r\n",
        "                                       n_heads=2,\r\n",
        "                                       mode='train'),\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    \r\n",
        "    return loop"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}