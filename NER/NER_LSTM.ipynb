{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_LSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOR56N1xwG6CB/ryiPZKeYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/NER/NER_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QdENeRCon94"
      },
      "source": [
        "# Named Entity Recognition(NER)\r\n",
        "In this Jupyter notebook, a NER using LSTM will be implemented. I will use Trax as the development library. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7_8SZEroYy3"
      },
      "source": [
        "import os\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq7xp1Q5v_GB",
        "outputId": "2cd73225-9c66-4816-ebb1-cda30a4ef2b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training\r\n",
        "\r\n",
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 522kB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 15.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 36.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 46.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 43.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.9MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2jJ2fJI3RV"
      },
      "source": [
        "## Data\r\n",
        "This section loads the data such as sentences, tags, words, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OKF5G29JD9-"
      },
      "source": [
        "# define corresponding files.\r\n",
        "sentences_file = \"./data/sentences.txt\"\r\n",
        "labels_file = \"./data/labels.txt\"\r\n",
        "words_file = \"./data/words.txt\"\r\n",
        "tags_file = \"./data/tags.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_waOw59Jhfs"
      },
      "source": [
        "### Sentences, Labels, Words, Tags\r\n",
        "Sentences, corresponding sequence of NER labels, unique words, and unique tags are loaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmtO4yprJue1"
      },
      "source": [
        "# load content from given file\r\n",
        "def load_content(file):\r\n",
        "  f = open(file, mode=\"r\", encoding=\"ISO-8859-1\")\r\n",
        "  return [line.replace(\"\\n\",\"\") for line in f.readlines()]\r\n",
        "\r\n",
        "# load sentences\r\n",
        "sentences = load_content(sentences_file)\r\n",
        "labels = load_content(labels_file)\r\n",
        "words = load_content(words_file)\r\n",
        "tags_raw = load_content(tags_file)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaSD_LJ1A08"
      },
      "source": [
        "### Vocabulary of Words and Tags\r\n",
        "In order to vectorize the sentences, it is required to build vocabulary of words, similarly for the tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pP091u1N_0"
      },
      "source": [
        "# build the vocabulary\r\n",
        "vocab = {words[i]:i for i in range(len(words))}\r\n",
        "\r\n",
        "# add <PAD> to vocab\r\n",
        "vocab['<PAD>'] = len(vocab)\r\n",
        "vocab['<UNK>'] = len(vocab)\r\n",
        "\r\n",
        "# build the tags vocab\r\n",
        "tags = {tags_raw[i]:i for i in range(len(tags_raw))}"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJ0PvF1LhT7"
      },
      "source": [
        "### Vectorize Sentences and Labels\r\n",
        "In this step, we need to vectorize the sentences and labels using the vocab and tag dictionaries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p70Cr8VMN-f"
      },
      "source": [
        "# vectorize sentences\r\n",
        "v_sentences = [ [vocab[t] if t in vocab else vocab['<UNK>'] for t in sentence.split(' ')] for sentence in sentences]\r\n",
        "\r\n",
        "# vectorize labels\r\n",
        "v_labels = [[tags[l] for l in label.split(' ')] for label in labels]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkmMCjV4_T2Y"
      },
      "source": [
        "### Train, Validation, Test split\r\n",
        "In this section, the sentences and corresponding label sequences are divived into train, validation, and test set. The split is based on ration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FshVEs_mHj"
      },
      "source": [
        "# define train/val/test retio(percentage)\r\n",
        "train_r, val_r, test_r = 70, 10, 20\r\n",
        "\r\n",
        "# find the end index for train split\r\n",
        "train_end_i = int(len(v_sentences) * train_r/100)\r\n",
        "\r\n",
        "# find the end index for validaition set. It located after the train set\r\n",
        "val_end_i = train_end_i + int(len(v_sentences) * val_r/100)\r\n",
        "\r\n",
        "# generate the train/val/test sentenes and label-sequences\r\n",
        "train_s, train_l = v_sentences[:train_end_i], v_labels[:train_end_i]\r\n",
        "val_s, val_l = v_sentences[train_end_i:val_end_i], v_labels[train_end_i:val_end_i]\r\n",
        "test_s, test_l = v_sentences[val_end_i:], v_labels[val_end_i]\r\n",
        "\r\n",
        "# assert the split\r\n",
        "assert len(v_sentences) == len(train_s) + len(val_s) + len(test_s)\r\n",
        "print(f'train size = {len(train_s)}, validation size = {len(val_s)}, test size = {len(test_s)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtpWW42GSCGe"
      },
      "source": [
        "### Data Generator\r\n",
        "Data generator is a key part of most on NLP applications using deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7JzNMj-9Sr"
      },
      "source": [
        "# Data generator\r\n",
        "def data_generator(x, y, batch_size, shuffle=False):\r\n",
        "  '''\r\n",
        "  Input:\r\n",
        "    x: list of inputs, each input is a sentence(sequence)\r\n",
        "    y: list of labels, each label is a sequence of tags\r\n",
        "    batch_size: num for the batch-size\r\n",
        "    shuffle: indicates if the shuffle is needed or not.\r\n",
        "  Output:\r\n",
        "    \r\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}