{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization_Transformers_Decoder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOJEsRqhDG3vSSd8krl+wOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/Summarization/TextSummarization_Transformers_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGZzcJSBmzUn"
      },
      "source": [
        "# Text Summarization using Transformers based on GPT (Decoder)\r\n",
        "This jupyter notebook implements a text summarization using Transformerts. It employes Trax to train a summarizer on cnn daily news. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2MCb29nt3m"
      },
      "source": [
        "## Install Libraries\r\n",
        "this section, required libraries such as Trax and SentencePiece will be installed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RYs5FgGn79W"
      },
      "source": [
        "# install SentencePiece\r\n",
        "!pip install sentencepiece\r\n",
        "\r\n",
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "\r\n",
        "# check the Trax version\r\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTCBHsmofO0"
      },
      "source": [
        "## Import Libraries\r\n",
        "Required libraries will be imported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWyOdC4msvX"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import random as rnd\r\n",
        "import textwrap\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.fastmath import numpy as jnp\r\n",
        "import sentencepiece as spm\r\n",
        "from trax.supervised import training"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4302NMiknjh5"
      },
      "source": [
        "## Initializations\r\n",
        "This section will initialize required parameters of libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSScCxRcnrrJ"
      },
      "source": [
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWJFEUvmqnnK"
      },
      "source": [
        "# Data Prepare\r\n",
        "In this section, the data will be prepared and converted to tensor in the required format for the Transformers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmvKY1tUR1y2"
      },
      "source": [
        "### Train SentencePiece and Build Vocabulary\r\n",
        "In this section, a vocabulary will be built using sentencepiece. It will contain sub-words. Generated model will be saved in *./data/mbpe.model*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBwWgSFRSBSs"
      },
      "source": [
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\r\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\r\n",
        "spm.SentencePieceTrainer.train('--input=./data/vocab_train.txt --model_prefix=./data/mbpe --model_type=bpe --user_defined_symbols=[SEP] --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS] --vocab_size=20000')\r\n",
        "\r\n",
        "# changind the pre-defined symbols\r\n",
        "#spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\r\n",
        "\r\n",
        "# makes segmenter instance and loads the model file (m.model)\r\n",
        "sp = spm.SentencePieceProcessor()\r\n",
        "sp.load('./data/mbpe.model')\r\n",
        "\r\n",
        "# get the size of vocabulary\r\n",
        "vocab_size = sp.get_piece_size()\r\n",
        "\r\n",
        "# get the vocabulary\r\n",
        "vocab_terms = [sp.id_to_piece(id) for id in range(vocab_size)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma-UL4tUsk74",
        "outputId": "eb1590f1-0b25-468a-c284-762e23feb530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('eos=', sp.eos_id())\r\n",
        "print('unk=', sp.unk_id())\r\n",
        "print('pad=', sp.pad_id())  \r\n",
        "print('code=',sp.decode_ids([0, 1, 2, 3, 4, 5, 6]))\r\n",
        "print('id = ',sp.encode_as_ids(\"[SEP]\"))\r\n",
        "print('text = ', sp.decode_ids([19930, 4]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eos= 3\n",
            "unk= 1\n",
            "pad= 0\n",
            "code=  â‡ [SEP] t a\n",
            "id =  [19930, 4]\n",
            "text =  [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD5xcd3pVtYd"
      },
      "source": [
        "### Test tokenization\r\n",
        "Here are an example to see whether the tokenization using sentencepiece is working or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMy3VRlZVVf1"
      },
      "source": [
        "# encode: text => id\r\n",
        "s1 = \"Four groups that advocate for immigrant rights said Thursday .\"\r\n",
        "\r\n",
        "# print the encoded results, tokens and ids\r\n",
        "print(sp.encode_as_pieces(s1))\r\n",
        "print(sp.encode_as_ids(s1))\r\n",
        "\r\n",
        "# print decoded \r\n",
        "print(sp.decode_pieces(sp.encode_as_pieces(s1)))\r\n",
        "print(sp.decode_ids([1758, 1093, 32, 21, 3370, 25, 8133, 485, 26, 222, 70, 58, 959]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy_WAIeeV993"
      },
      "source": [
        "### Data Load\r\n",
        "In this section, the data will be loaded. Data is stored in csv file using two columns of *body* and *highlights*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsjEvs-rKpY"
      },
      "source": [
        "# data file\r\n",
        "cnn_file = \"./data/cnn_1000.csv\"\r\n",
        "\r\n",
        "# train, validation, test rates\r\n",
        "train_rate, val_rate, test_rate = 80, 10, 10\r\n",
        "\r\n",
        "# read data\r\n",
        "cnn = pd.read_csv(cnn_file, sep=\"\\t\")\r\n",
        "cnn_articles = cnn[\"body\"]\r\n",
        "cnn_highlights = cnn[\"highlights\"]\r\n",
        "\r\n",
        "# get boundaries\r\n",
        "total_samples = len(cnn_articles)\r\n",
        "train_end_i = int( total_samples * train_rate/100)\r\n",
        "val_end_i = train_end_i + int(total_samples * val_rate/100)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-5zT08J4F99"
      },
      "source": [
        "### Data Vectorize\r\n",
        "It is required to vectorize the articles as well as their highlights. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy1weXA25Kdj"
      },
      "source": [
        "# Vectorize articles and highlights\r\n",
        "articles = [sp.encode_as_ids(artcl) for artcl in cnn_articles]\r\n",
        "highlights = [sp.encode_as_ids(hglt) for hglt in cnn_highlights]\r\n",
        "\r\n",
        "# split into train, validation, and test sets\r\n",
        "train_articles, val_articles, test_articles = articles[:train_end_i], articles[train_end_i:val_end_i], articles[val_end_i:]  \r\n",
        "train_highlights, val_highlights, test_highlights = highlights[:train_end_i], highlights[train_end_i:val_end_i], highlights[val_end_i:]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hzx41xkFoJe"
      },
      "source": [
        "### Max Length\r\n",
        "It is needed to know the max lenght of concatenations of articles and their highlights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOw_-HkYFzUz"
      },
      "source": [
        "# find max length\r\n",
        "max_l = 0\r\n",
        "for a,h in zip(articles, highlights):\r\n",
        "  if len(a)+len(h) > max_l:\r\n",
        "    max_l = len(a)+len(h)\r\n",
        "\r\n",
        "# add the EOS, and SEP to the length: article + [EOS] + [SEP] + highlights + [EOS]\r\n",
        "max_l += 3"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXsmm6dU5TMm",
        "outputId": "9da44ead-1610-4c92-cc02-1bf3c9e25d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_l"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2611"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDWy-rF7pWOD"
      },
      "source": [
        "### Data Generator\r\n",
        "Data generators provides data for training, validation, and testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scPETxm3pfQj"
      },
      "source": [
        "# define the data generator\r\n",
        "def data_generator(x, y, max_length, batch_size, eos_id, sep_id, pad_id, loop = False, shuffle=False):\r\n",
        "  '''\r\n",
        "  Yield list of trainig inputs, training targets, and training mask with given batch-size.\r\n",
        "  Inputs:\r\n",
        "    x: list of articles\r\n",
        "    y: list of highlights\r\n",
        "    max_length: max length of concatenation of an article and its highlights\r\n",
        "    loop: indicates whether it make loop at the end of list or not.\r\n",
        "    shuffle: shows if the shuffle is required.\r\n",
        "  Outputs:\r\n",
        "    triples of (input, traget, mask)\r\n",
        "  '''\r\n",
        "  data_size = len(x)\r\n",
        "  indexes = [*range(data_size)]\r\n",
        "  \r\n",
        "  # shuffle if required\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(indexes)\r\n",
        "\r\n",
        "  index = 0\r\n",
        "  stop = False\r\n",
        "\r\n",
        "  # iterate\r\n",
        "  while True:\r\n",
        "    \r\n",
        "    # primary indexes for outputs\r\n",
        "    out_indexes = [0]*batch_size\r\n",
        "\r\n",
        "    # iterate batch_size time\r\n",
        "    for i in range(batch_size):\r\n",
        "      \r\n",
        "      # if at the end of list\r\n",
        "      if index >= data_size:\r\n",
        "        if not Loop:\r\n",
        "          stop = True\r\n",
        "          break\r\n",
        "        \r\n",
        "        index = 0\r\n",
        "        if shuffle:\r\n",
        "          rnd.shuffle(indexes)\r\n",
        "\r\n",
        "      # add next on to the output indexes\r\n",
        "      out_indexes[i] = indexes[index]\r\n",
        "\r\n",
        "      # increase index\r\n",
        "      index += 1\r\n",
        "\r\n",
        "    # convert the output indexes into actual output\r\n",
        "    # pad all to the max-length\r\n",
        "    #X = np.full((batch_size, max_length), pad_id)\r\n",
        "    #Y = np.full((batch_size, max_length), pad_id)\r\n",
        "    result_x = []\r\n",
        "    result_mask = []\r\n",
        "\r\n",
        "    # interate through the batches\r\n",
        "    for i in range(batch_size):\r\n",
        "\r\n",
        "      # add the article_vec + eos_id + pad_id + highlight_vec + eos_id\r\n",
        "      out = x[out_indexes[i]] + [eos_id] + [pad_id] + y[out_indexes[i]] + [eos_id]\r\n",
        "      mask_temp = [0]*(len(x[out_indexes[i]]) + 2) + [1]*(len(y[out_indexes[i]]) + 1)\r\n",
        "\r\n",
        "      # pad to the max_length\r\n",
        "      out = out + [pad_id] * (max_length-len(out))\r\n",
        "      mask_temp = mask_temp + [pad_id] * (max_length-len(mask_temp))\r\n",
        "\r\n",
        "      # add the results\r\n",
        "      result_x.append(out)\r\n",
        "      result_mask.append(mask_temp)\r\n",
        "\r\n",
        "    # yield the result\r\n",
        "    yield jnp.array(result_x), jnp.array(result_x), jnp.array(result_mask)\r\n",
        "\r\n",
        "    # is stop flagged, do not continue\r\n",
        "    if stop:\r\n",
        "      break"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvxQX1ynFoXG",
        "outputId": "1cfad1be-8e91-4ce9-cf0e-92fd48f87a33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "eos_id = sp.eos_id()\r\n",
        "pad_id = sp.pad_id()\r\n",
        "sep_id = 4\r\n",
        "\r\n",
        "temp_data_generator = data_generator(train_articles, train_highlights, max_length = max_l, batch_size=4, eos_id=eos_id, sep_id=sep_id, pad_id=pad_id)\r\n",
        "next_batch = next(temp_data_generator)\r\n",
        "x,y,mask = next_batch\r\n",
        "print(f' x_shape = {x.shape}, y_shape={y.shape}, mask shape = {mask.shape}')\r\n",
        "print(f'x[0] = {type(x[0])}')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " x_shape = (4, 2611), y_shape=(4, 2611), mask shape = (4, 2611)\n",
            "x[0] = <class 'jax.interpreters.xla._DeviceArray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2JZWmJSreZe"
      },
      "source": [
        "# Transformer - Decoder Only\r\n",
        "This experiment is based on using only the Decoder part of Transformer. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tS8jpoNrg1Q"
      },
      "source": [
        "## Embedding and Positional Encoding\r\n",
        "In this section, an Embedding and Positional Encoding will be implemented. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKqk3FcanF1W"
      },
      "source": [
        "def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\r\n",
        "    \"\"\"Returns a list of layers that: \r\n",
        "    1. takes a block of text as input, \r\n",
        "    2. embeds the words in that text, and \r\n",
        "    3. adds positional encoding, \r\n",
        "       i.e. associates a number in range(max_len) with \r\n",
        "       each word in each sentence of embedded input text \r\n",
        "    \r\n",
        "    The input is a list of tokenized blocks of text\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "    \"\"\"\r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    return [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),  \r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)] "
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JHSv8fsrrAJ"
      },
      "source": [
        "## Feedforward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23W5eUxkmxGQ"
      },
      "source": [
        "def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a feed-forward block.\r\n",
        "    The input is an activation tensor.\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\r\n",
        "    return [ \r\n",
        "        # Normalize layer inputs\r\n",
        "        tl.LayerNorm(), \r\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_ff), \r\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\r\n",
        "        ff_activation(),  # Generally ReLU\r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode), \r\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\r\n",
        "        tl.Dense(d_model), \r\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\r\n",
        "        tl.Dropout(rate=dropout, mode=mode) \r\n",
        "    ]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WXFhoS0rxDN"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCPmTpynLzO"
      },
      "source": [
        "def DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation):\r\n",
        "    \"\"\"\r\n",
        "    Returns a list of layers that implements a Transformer decoder block.\r\n",
        "    The input is an activation tensor.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        mode (str): 'train' or 'eval'.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\r\n",
        "    \"\"\"\r\n",
        "        \r\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\r\n",
        "    return [\r\n",
        "      tl.Residual(\r\n",
        "          # Normalize layer input\r\n",
        "          tl.LayerNorm(), \r\n",
        "          # Add causal attention \r\n",
        "          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode),\r\n",
        "          # Add dropout with rate and mode specified\r\n",
        "          tl.Dropout(rate=dropout, mode=mode)\r\n",
        "        ),\r\n",
        "      tl.Residual(\r\n",
        "          # Add feed-forward block\r\n",
        "          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\r\n",
        "          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\r\n",
        "        ),\r\n",
        "      ]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypWsN6XopLhx"
      },
      "source": [
        "## Transformer LM\r\n",
        "In this section, a decoder-based Transformer is implemented.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOS0JifkpUPu"
      },
      "source": [
        "# A Transformer using only the Docoder part\r\n",
        "def TransformerLM(vocab_size=33300,\r\n",
        "                  d_model=4,\r\n",
        "                  d_ff=16,\r\n",
        "                  n_layers=2,\r\n",
        "                  n_heads=2,\r\n",
        "                  dropout=0.1,\r\n",
        "                  max_len=4096,\r\n",
        "                  mode='train',\r\n",
        "                  ff_activation=tl.Relu):\r\n",
        "    \"\"\"\r\n",
        "    Returns a Transformer language model.\r\n",
        "    The input to the model is a tensor of tokens. \r\n",
        "\r\n",
        "    Args:\r\n",
        "        vocab_size (int): vocab size.\r\n",
        "        d_model (int):  depth of embedding.\r\n",
        "        d_ff (int): depth of feed-forward layer.\r\n",
        "        n_layers (int): number of decoder layers.\r\n",
        "        n_heads (int): number of attention heads.\r\n",
        "        dropout (float): dropout rate (how much to drop out).\r\n",
        "        max_len (int): maximum symbol length for positional encoding.\r\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\r\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\r\n",
        "        to activations over a vocab set.\r\n",
        "    \"\"\"\r\n",
        "  \r\n",
        "    # Embedding inputs and positional encoder\r\n",
        "    positional_encoder = [ \r\n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\r\n",
        "        tl.Embedding(vocab_size, d_model),\r\n",
        "        # Use dropout with rate and mode specified\r\n",
        "        tl.Dropout(rate=dropout,mode=mode),\r\n",
        "        # Add positional encoding layer with maximum input length and mode specified\r\n",
        "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\r\n",
        "\r\n",
        "    # Create the complete model as written in the figure\r\n",
        "    return tl.Serial(\r\n",
        "        # Use teacher forcing (feed output of previous step to current step)\r\n",
        "        tl.ShiftRight(mode=mode), # Specify the mode!\r\n",
        "        # Add positional encoder\r\n",
        "        positional_encoder,\r\n",
        "        # Add decoder blocks,(list of Decoders)\r\n",
        "        [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)],\r\n",
        "        # Normalize layer\r\n",
        "        tl.LayerNorm(),\r\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\r\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\r\n",
        "        tl.Dense(vocab_size),\r\n",
        "        # Get probabilities with Logsoftmax\r\n",
        "        tl.LogSoftmax()\r\n",
        "    )"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5I-P44L1hQP"
      },
      "source": [
        "# Training\r\n",
        "In this section, implemented Decoder-based Transformer will be trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hv5zf0u2jpI"
      },
      "source": [
        "# training loop\r\n",
        "def training_loop(model, train_gen, eval_gen, train_steps=1, output_dir = \"model\"):\r\n",
        "    '''\r\n",
        "    Input:\r\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\r\n",
        "        train_gen (generator): Training stream of data.\r\n",
        "        eval_gen (generator): Evaluation stream of data.\r\n",
        "        train_steps: number of steps \r\n",
        "        output_dir (str): folder to save your file.\r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop.\r\n",
        "    '''\r\n",
        "    #output_dir = os.path.expanduser(output_dir)  # trainer is an object\r\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\r\n",
        "\r\n",
        "    # define the training task\r\n",
        "    train_task = training.TrainTask( \r\n",
        "      labeled_data= train_gen, # The training generator\r\n",
        "      loss_layer= tl.CrossEntropyLoss(), # Loss function \r\n",
        "      optimizer= trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\r\n",
        "      lr_schedule= lr_schedule,\r\n",
        "      n_steps_per_checkpoint=100\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the evaluation task\r\n",
        "    eval_task = training.EvalTask( \r\n",
        "      labeled_data= eval_gen, # The evaluation generator\r\n",
        "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # CrossEntropyLoss and Accuracy\r\n",
        "      n_eval_batches = 10         # Number of batches to use on each evaluation\r\n",
        "    )\r\n",
        "\r\n",
        "    # define the training loop\r\n",
        "    loop = training.Loop(\r\n",
        "        model,\r\n",
        "        train_task, # A train task\r\n",
        "        eval_tasks = [eval_task], # The evaluation task\r\n",
        "        output_dir=output_dir)\r\n",
        "    \r\n",
        "    # run the training loop for \r\n",
        "    loop.run(n_steps= train_steps)\r\n",
        "    return loop"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG8CWtQIiDuY"
      },
      "source": [
        "### Training Process\r\n",
        "It is needed to instantiate train and validation data generator as well as training steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xssJg1dhIDB"
      },
      "source": [
        "train_steps = 10           # number of training steps\r\n",
        "!rm -f 'model/model.pkl.gz'   # Remove old model.pkl if it exists\r\n",
        "\r\n",
        "# train data generator\r\n",
        "train_data_gen = data_generator(train_articles, train_highlights, \r\n",
        "                                max_length = max_l, batch_size=64, \r\n",
        "                                eos_id=eos_id, sep_id=sep_id, pad_id=pad_id)\r\n",
        "\r\n",
        "# validation data generator\r\n",
        "val_data_gen = data_generator(val_articles, val_highlights, \r\n",
        "                              max_length = max_l, batch_size=64, \r\n",
        "                              eos_id=eos_id, sep_id=sep_id, pad_id=pad_id)\r\n",
        "\r\n",
        "# define the model\r\n",
        "model = TransformerLM(vocab_size=2000)\r\n",
        "\r\n",
        "# instantiate the training loop\r\n",
        "#train_loops = training_loop(model, train_data_gen, val_data_gen, train_steps=train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi018E0Yz_vP"
      },
      "source": [
        "# Conclusion\r\n",
        "I was not able to train the model because of memory limits."
      ]
    }
  ]
}