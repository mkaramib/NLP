{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_LSTM.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNUEyuXdkDObz0ZIeYgAza5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/NER/NER_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QdENeRCon94"
      },
      "source": [
        "# Named Entity Recognition(NER)\r\n",
        "In this Jupyter notebook, a NER using LSTM will be implemented. I will use Trax as the development library. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7_8SZEroYy3"
      },
      "source": [
        "import os\r\n",
        "import random as rnd\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq7xp1Q5v_GB"
      },
      "source": [
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training\r\n",
        "\r\n",
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as jax_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2jJ2fJI3RV"
      },
      "source": [
        "## Data\r\n",
        "This section loads the data such as sentences, tags, words, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OKF5G29JD9-"
      },
      "source": [
        "# define corresponding files.\r\n",
        "sentences_file = \"./data/sentences.txt\"\r\n",
        "labels_file = \"./data/labels.txt\"\r\n",
        "words_file = \"./data/words.txt\"\r\n",
        "tags_file = \"./data/tags.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_waOw59Jhfs"
      },
      "source": [
        "### Sentences, Labels, Words, Tags\r\n",
        "Sentences, corresponding sequence of NER labels, unique words, and unique tags are loaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmtO4yprJue1"
      },
      "source": [
        "# load content from given file\r\n",
        "def load_content(file):\r\n",
        "  f = open(file, mode=\"r\", encoding=\"ISO-8859-1\")\r\n",
        "  return [line.replace(\"\\n\",\"\") for line in f.readlines()]\r\n",
        "\r\n",
        "# load sentences\r\n",
        "sentences = load_content(sentences_file)\r\n",
        "labels = load_content(labels_file)\r\n",
        "words = load_content(words_file)\r\n",
        "tags_raw = load_content(tags_file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaSD_LJ1A08"
      },
      "source": [
        "### Vocabulary of Words and Tags\r\n",
        "In order to vectorize the sentences, it is required to build vocabulary of words, similarly for the tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pP091u1N_0"
      },
      "source": [
        "# build the vocabulary\r\n",
        "vocab = {words[i]:i for i in range(len(words))}\r\n",
        "\r\n",
        "# add <PAD> to vocab\r\n",
        "vocab['<PAD>'] = len(vocab)\r\n",
        "vocab['<UNK>'] = len(vocab)\r\n",
        "\r\n",
        "# build the tags vocab\r\n",
        "tags = {tags_raw[i]:i for i in range(len(tags_raw))}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJ0PvF1LhT7"
      },
      "source": [
        "### Vectorize Sentences and Labels\r\n",
        "In this step, we need to vectorize the sentences and labels using the vocab and tag dictionaries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p70Cr8VMN-f"
      },
      "source": [
        "# vectorize sentences\r\n",
        "v_sentences = [ [vocab[t] if t in vocab else vocab['<UNK>'] for t in sentence.split(' ')] for sentence in sentences]\r\n",
        "\r\n",
        "# vectorize labels\r\n",
        "v_labels = [[tags[l] for l in label.split(' ')] for label in labels]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkmMCjV4_T2Y"
      },
      "source": [
        "### Train, Validation, Test split\r\n",
        "In this section, the sentences and corresponding label sequences are divived into train, validation, and test set. The split is based on ration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FshVEs_mHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00dba86-061b-41a2-ac1e-e0cd7445f7c6"
      },
      "source": [
        "# define train/val/test retio(percentage)\r\n",
        "train_r, val_r, test_r = 70, 10, 20\r\n",
        "\r\n",
        "# find the end index for train split\r\n",
        "train_end_i = int(len(v_sentences) * train_r/100)\r\n",
        "\r\n",
        "# find the end index for validaition set. It located after the train set\r\n",
        "val_end_i = train_end_i + int(len(v_sentences) * val_r/100)\r\n",
        "\r\n",
        "# generate the train/val/test sentenes and label-sequences\r\n",
        "train_s, train_l = v_sentences[:train_end_i], v_labels[:train_end_i]\r\n",
        "val_s, val_l = v_sentences[train_end_i:val_end_i], v_labels[train_end_i:val_end_i]\r\n",
        "test_s, test_l = v_sentences[val_end_i:], v_labels[val_end_i]\r\n",
        "\r\n",
        "# assert the split\r\n",
        "assert len(v_sentences) == len(train_s) + len(val_s) + len(test_s)\r\n",
        "print(f'train size = {len(train_s)}, validation size = {len(val_s)}, test size = {len(test_s)}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size = 33570, validation size = 4795, test size = 9593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtpWW42GSCGe"
      },
      "source": [
        "### Data Generator\r\n",
        "Data generator is a key part of most on NLP applications using deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7JzNMj-9Sr"
      },
      "source": [
        "# Data generator\r\n",
        "def data_generator(x, y, batch_size, pad, shuffle=False):\r\n",
        "  '''\r\n",
        "  Input:\r\n",
        "    x: list of inputs, each input is a sentence(sequence)\r\n",
        "    y: list of labels, each label is a sequence of tags\r\n",
        "    batch_size: num for the batch-size\r\n",
        "    pad: word id for the <PAD> in the vocab.\r\n",
        "    shuffle: indicates if the shuffle is needed or not.\r\n",
        "  Output:\r\n",
        "    \r\n",
        "  '''\r\n",
        "  l = len(x)\r\n",
        "  x_indexes = [*range(l)]\r\n",
        "\r\n",
        "  # shuffle the data if required\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(x_indexes)\r\n",
        "\r\n",
        "  index = 0\r\n",
        "  while True:\r\n",
        "      \r\n",
        "      # max length of sentence\r\n",
        "      max_l = 0\r\n",
        "\r\n",
        "      # instaniate output indexes\r\n",
        "      x_out, y_out = [0]*batch_size, [0]*batch_size\r\n",
        "\r\n",
        "      # select a list of size of batch.\r\n",
        "      for i in range(batch_size):\r\n",
        "        # at the end of data, reset the index\r\n",
        "        if index >= l:\r\n",
        "          index = 0\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(x_indexes)\r\n",
        "        \r\n",
        "        # add to the \r\n",
        "        x_out[i] = x[x_indexes[index]]\r\n",
        "        y_out[i] = y[x_indexes[index]]\r\n",
        "      \r\n",
        "        # check max row\r\n",
        "        lenx = len(x_out[i])\r\n",
        "        if lenx > max_l:\r\n",
        "          max_l = lenx\r\n",
        "        \r\n",
        "        # increase the index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "      # convert to the output \r\n",
        "      X = np.full((batch_size, max_l), pad)\r\n",
        "      Y = np.full((batch_size, max_l), pad)\r\n",
        "      for i in range(batch_size):\r\n",
        "        for j in range(len(x_out[i])):\r\n",
        "          X[i, j] = x_out[i][j]\r\n",
        "          Y[i, j] = y_out[i][j]\r\n",
        "      \r\n",
        "      # yield the result\r\n",
        "      yield((X,Y))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkqaYIwM3oyB"
      },
      "source": [
        "## Model\r\n",
        "This section desing a LSTM based model for the NER.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6zEL9Yz3uER"
      },
      "source": [
        "# NER as the NN model.\r\n",
        "def NER(vocab_size=35181, d_model=50, tags=tags):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        vocab_size - integer containing the size of the vocabulary\r\n",
        "        d_model - integer describing the embedding size\r\n",
        "      Output:\r\n",
        "        model - a trax serial model\r\n",
        "    '''\r\n",
        "    # define the model\r\n",
        "    model = tl.Serial(\r\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Embedding layer\r\n",
        "      tl.LSTM(n_units=d_model),     # LSTM layer\r\n",
        "      tl.Dense(n_units=len(tags)),  # Dense layer with len(tags) units\r\n",
        "      tl.LogSoftmax()               # LogSoftmax layer\r\n",
        "      )\r\n",
        "     \r\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H0b86ngD7ek"
      },
      "source": [
        "### Training/Validation Generators\r\n",
        "In this section, training and validation data are generated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbT3Aht3EEMu"
      },
      "source": [
        "rnd.seed(33)\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "# Create training data\r\n",
        "train_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(train_s, train_l, batch_size=batch_size, pad=vocab['<PAD>'], shuffle=True),\r\n",
        "    id_to_mask=vocab['<PAD>'])\r\n",
        "\r\n",
        "# Create validation data\r\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(val_s, val_l, batch_size=batch_size, pad=vocab['<PAD>'], shuffle=True),\r\n",
        "    id_to_mask=vocab['<PAD>'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_wdE-joFJ_h"
      },
      "source": [
        "### Training\r\n",
        "In this section, the training tasks and loop will be build. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEu8XbN9HtQv"
      },
      "source": [
        "# training loop\r\n",
        "def train_model(NER_model, train_generator, eval_generator, train_steps=1, output_dir='model'):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        NER_model - the model you are building\r\n",
        "        train_generator - The data generator for training examples\r\n",
        "        eval_generator - The data generator for validation examples,\r\n",
        "        train_steps - number of training steps\r\n",
        "        output_dir - folder to save your model\r\n",
        "    Output:\r\n",
        "        training_loop - a trax supervised training Loop\r\n",
        "    '''\r\n",
        "    # step 1- training task\r\n",
        "    train_task = training.TrainTask(\r\n",
        "      train_generator,                    # A train data generator\r\n",
        "      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\r\n",
        "      optimizer = trax.optimizers.Adam(0.01),  # The adam optimizer\r\n",
        "      n_steps_per_checkpoint=200, #This will print the results at every 200 training steps.\r\n",
        "    )\r\n",
        "\r\n",
        "    # step 2- evaluation task\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "      labeled_data = eval_generator,      # A labeled data generator\r\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\r\n",
        "      n_eval_batches = 10         # Number of batches to use on each evaluation\r\n",
        "    )\r\n",
        "\r\n",
        "    # step 3- training loop\r\n",
        "    training_loop = training.Loop(\r\n",
        "        NER_model,# A model to train\r\n",
        "        train_task, # A train task\r\n",
        "        eval_tasks = [eval_task], # The evaluation task\r\n",
        "        output_dir = output_dir) # The output directory\r\n",
        "\r\n",
        "    # run with train_steps\r\n",
        "    training_loop.run(n_steps = train_steps)\r\n",
        "\r\n",
        "    # return the training loop\r\n",
        "    return training_loop"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj3Xav_lIjpf"
      },
      "source": [
        "### Test the training\r\n",
        "In the following code, the training is tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh9j46ZQp_v8"
      },
      "source": [
        "train_steps = 1000            # In coursera we can only train 100 steps\r\n",
        "!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\r\n",
        "\r\n",
        "# Train the model\r\n",
        "training_loop = train_model(NER(), train_generator, eval_generator, train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}