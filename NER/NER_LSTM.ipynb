{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_LSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOBwXo3NXqjP7s2RH+dR9jK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/NER/NER_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QdENeRCon94"
      },
      "source": [
        "# Named Entity Recognition(NER)\r\n",
        "In this Jupyter notebook, a NER using LSTM will be implemented. I will use Trax as the development library. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7_8SZEroYy3"
      },
      "source": [
        "import os\r\n",
        "import random as rnd\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq7xp1Q5v_GB"
      },
      "source": [
        "# install Trax\r\n",
        "!pip install -q -U trax\r\n",
        "import trax\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids\r\n",
        "from trax.supervised import training\r\n",
        "\r\n",
        "# import trax.fastmath.numpy\r\n",
        "import trax.fastmath.numpy as jax_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq2jJ2fJI3RV"
      },
      "source": [
        "## Data\r\n",
        "This section loads the data such as sentences, tags, words, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OKF5G29JD9-"
      },
      "source": [
        "# define corresponding files.\r\n",
        "sentences_file = \"./data/sentences.txt\"\r\n",
        "labels_file = \"./data/labels.txt\"\r\n",
        "words_file = \"./data/words.txt\"\r\n",
        "tags_file = \"./data/tags.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_waOw59Jhfs"
      },
      "source": [
        "### Sentences, Labels, Words, Tags\r\n",
        "Sentences, corresponding sequence of NER labels, unique words, and unique tags are loaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmtO4yprJue1"
      },
      "source": [
        "# load content from given file\r\n",
        "def load_content(file):\r\n",
        "  f = open(file, mode=\"r\", encoding=\"ISO-8859-1\")\r\n",
        "  return [line.replace(\"\\n\",\"\") for line in f.readlines()]\r\n",
        "\r\n",
        "# load sentences\r\n",
        "sentences = load_content(sentences_file)\r\n",
        "labels = load_content(labels_file)\r\n",
        "words = load_content(words_file)\r\n",
        "tags_raw = load_content(tags_file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaSD_LJ1A08"
      },
      "source": [
        "### Vocabulary of Words and Tags\r\n",
        "In order to vectorize the sentences, it is required to build vocabulary of words, similarly for the tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pP091u1N_0"
      },
      "source": [
        "# build the vocabulary\r\n",
        "vocab = {words[i]:i for i in range(len(words))}\r\n",
        "\r\n",
        "# add <PAD> to vocab\r\n",
        "vocab['<PAD>'] = len(vocab)\r\n",
        "vocab['<UNK>'] = len(vocab)\r\n",
        "\r\n",
        "# build the tags vocab\r\n",
        "tags = {tags_raw[i]:i for i in range(len(tags_raw))}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJ0PvF1LhT7"
      },
      "source": [
        "### Vectorize Sentences and Labels\r\n",
        "In this step, we need to vectorize the sentences and labels using the vocab and tag dictionaries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p70Cr8VMN-f"
      },
      "source": [
        "# vectorize sentences\r\n",
        "v_sentences = [ [vocab[t] if t in vocab else vocab['<UNK>'] for t in sentence.split(' ')] for sentence in sentences]\r\n",
        "\r\n",
        "# vectorize labels\r\n",
        "v_labels = [[tags[l] for l in label.split(' ')] for label in labels]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkmMCjV4_T2Y"
      },
      "source": [
        "### Train, Validation, Test split\r\n",
        "In this section, the sentences and corresponding label sequences are divived into train, validation, and test set. The split is based on ration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8FshVEs_mHj"
      },
      "source": [
        "# define train/val/test retio(percentage)\r\n",
        "train_r, val_r, test_r = 70, 10, 20\r\n",
        "\r\n",
        "# find the end index for train split\r\n",
        "train_end_i = int(len(v_sentences) * train_r/100)\r\n",
        "\r\n",
        "# find the end index for validaition set. It located after the train set\r\n",
        "val_end_i = train_end_i + int(len(v_sentences) * val_r/100)\r\n",
        "\r\n",
        "# generate the train/val/test sentenes and label-sequences\r\n",
        "train_s, train_l = v_sentences[:train_end_i], v_labels[:train_end_i]\r\n",
        "val_s, val_l = v_sentences[train_end_i:val_end_i], v_labels[train_end_i:val_end_i]\r\n",
        "test_s, test_l = v_sentences[val_end_i:], v_labels[val_end_i:]\r\n",
        "\r\n",
        "# assert the split\r\n",
        "assert len(v_sentences) == len(train_s) + len(val_s) + len(test_s)\r\n",
        "print(f'train size = {len(train_s)}, validation size = {len(val_s)}, test size = {len(test_s)}')\r\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtpWW42GSCGe"
      },
      "source": [
        "### Data Generator\r\n",
        "Data generator is a key part of most on NLP applications using deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf7JzNMj-9Sr"
      },
      "source": [
        "# Data generator\r\n",
        "def data_generator(x, y, batch_size, pad, shuffle=False, loop=False):\r\n",
        "  '''\r\n",
        "  Input:\r\n",
        "    x: list of inputs, each input is a sentence(sequence)\r\n",
        "    y: list of labels, each label is a sequence of tags\r\n",
        "    batch_size: num for the batch-size\r\n",
        "    pad: word id for the <PAD> in the vocab.\r\n",
        "    shuffle: indicates if the shuffle is needed or not.\r\n",
        "  Output:\r\n",
        "    \r\n",
        "  '''\r\n",
        "  l = len(x)\r\n",
        "  x_indexes = [*range(l)]\r\n",
        "\r\n",
        "  # shuffle the data if required\r\n",
        "  if shuffle:\r\n",
        "    rnd.shuffle(x_indexes)\r\n",
        "\r\n",
        "  # define the stop flag\r\n",
        "  stop = False\r\n",
        "\r\n",
        "  index = 0\r\n",
        "  while True:\r\n",
        "      \r\n",
        "      # max length of sentence\r\n",
        "      max_l = 0\r\n",
        "\r\n",
        "      # instaniate output indexes\r\n",
        "      x_out, y_out = [0]*batch_size, [0]*batch_size\r\n",
        "\r\n",
        "      # select a list of size of batch.\r\n",
        "      for i in range(batch_size):\r\n",
        "        # at the end of data, reset the index\r\n",
        "        if index >= l:\r\n",
        "          if not loop:\r\n",
        "            stop = True\r\n",
        "\r\n",
        "          index = 0\r\n",
        "          if shuffle:\r\n",
        "            rnd.shuffle(x_indexes)\r\n",
        "        \r\n",
        "        # add to the \r\n",
        "        x_out[i] = x[x_indexes[index]]\r\n",
        "        y_out[i] = y[x_indexes[index]]\r\n",
        "      \r\n",
        "        # check max row\r\n",
        "        lenx = len(x_out[i])\r\n",
        "        if lenx > max_l:\r\n",
        "          max_l = lenx\r\n",
        "        \r\n",
        "        # increase the index\r\n",
        "        index += 1\r\n",
        "\r\n",
        "      # convert to the output \r\n",
        "      X = np.full((batch_size, max_l), pad)\r\n",
        "      Y = np.full((batch_size, max_l), pad)\r\n",
        "      for i in range(batch_size):\r\n",
        "        for j in range(len(x_out[i])):\r\n",
        "          X[i, j] = x_out[i][j]\r\n",
        "          Y[i, j] = y_out[i][j]\r\n",
        "      \r\n",
        "      # yield the result\r\n",
        "      yield((X,Y))\r\n",
        "\r\n",
        "      # if stop is flagged, return\r\n",
        "      if stop:\r\n",
        "        break"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkqaYIwM3oyB"
      },
      "source": [
        "## Training\r\n",
        "This section describes required steps to desing NN model, train generator, training process, and testing the model.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjiUKrHuaEDz"
      },
      "source": [
        "### NN Model\r\n",
        "This section desing a LSTM based model for the NER.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6zEL9Yz3uER"
      },
      "source": [
        "# NER as the NN model.\r\n",
        "def NER(vocab_size=35181, d_model=50, tags=tags):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        vocab_size - integer containing the size of the vocabulary\r\n",
        "        d_model - integer describing the embedding size\r\n",
        "      Output:\r\n",
        "        model - a trax serial model\r\n",
        "    '''\r\n",
        "    # define the model\r\n",
        "    model = tl.Serial(\r\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Embedding layer\r\n",
        "      tl.LSTM(n_units=d_model),     # LSTM layer\r\n",
        "      tl.Dense(n_units=len(tags)),  # Dense layer with len(tags) units\r\n",
        "      tl.LogSoftmax()               # LogSoftmax layer\r\n",
        "      )\r\n",
        "     \r\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H0b86ngD7ek"
      },
      "source": [
        "### Training/Validation Generators\r\n",
        "In this section, training and validation data are generated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbT3Aht3EEMu"
      },
      "source": [
        "rnd.seed(33)\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "# Create training data\r\n",
        "train_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(train_s, train_l, batch_size=batch_size, pad=vocab['<PAD>'], shuffle=True),\r\n",
        "    id_to_mask=vocab['<PAD>'])\r\n",
        "\r\n",
        "# Create validation data\r\n",
        "eval_generator = trax.data.inputs.add_loss_weights(\r\n",
        "    data_generator(val_s, val_l, batch_size=batch_size, pad=vocab['<PAD>'], shuffle=True),\r\n",
        "    id_to_mask=vocab['<PAD>'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_wdE-joFJ_h"
      },
      "source": [
        "### Train Method\r\n",
        "In this section, the training tasks and loop will be build. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEu8XbN9HtQv"
      },
      "source": [
        "# training loop\r\n",
        "def train_model(NER_model, train_generator, eval_generator, train_steps=1, output_dir='model'):\r\n",
        "    '''\r\n",
        "    Input: \r\n",
        "        NER_model - the model you are building\r\n",
        "        train_generator - The data generator for training examples\r\n",
        "        eval_generator - The data generator for validation examples,\r\n",
        "        train_steps - number of training steps\r\n",
        "        output_dir - folder to save your model\r\n",
        "    Output:\r\n",
        "        training_loop - a trax supervised training Loop\r\n",
        "    '''\r\n",
        "    # step 1- training task\r\n",
        "    train_task = training.TrainTask(\r\n",
        "      train_generator,                    # A train data generator\r\n",
        "      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\r\n",
        "      optimizer = trax.optimizers.Adam(0.01),  # The adam optimizer\r\n",
        "      n_steps_per_checkpoint=200, #This will print the results at every 200 training steps.\r\n",
        "    )\r\n",
        "\r\n",
        "    # step 2- evaluation task\r\n",
        "    eval_task = training.EvalTask(\r\n",
        "      labeled_data = eval_generator,      # A labeled data generator\r\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\r\n",
        "      n_eval_batches = 10         # Number of batches to use on each evaluation\r\n",
        "    )\r\n",
        "\r\n",
        "    # step 3- training loop\r\n",
        "    training_loop = training.Loop(\r\n",
        "        NER_model,# A model to train\r\n",
        "        train_task, # A train task\r\n",
        "        eval_tasks = [eval_task], # The evaluation task\r\n",
        "        output_dir = output_dir) # The output directory\r\n",
        "\r\n",
        "    # run with train_steps\r\n",
        "    training_loop.run(n_steps = train_steps)\r\n",
        "\r\n",
        "    # return the training loop\r\n",
        "    return training_loop"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj3Xav_lIjpf"
      },
      "source": [
        "### Training Process\r\n",
        "In the following code, the training process is instantiated and accomplished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh9j46ZQp_v8"
      },
      "source": [
        "train_steps = 1000            # In coursera we can only train 100 steps\r\n",
        "!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\r\n",
        "\r\n",
        "# Train the model\r\n",
        "training_loop = train_model(NER(), train_generator, eval_generator, train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohf_cluJbIM4"
      },
      "source": [
        "## Evaluation\r\n",
        "In the following, the evaluation steps are described."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKvX5rXXbd7T"
      },
      "source": [
        "### Prediction Sample\r\n",
        "In this section, an example for the prediction is presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiOW85Q6bn9-"
      },
      "source": [
        "# step 1: instantiate a test data generator\r\n",
        "test_data_generator = data_generator(test_s[:10], test_l[:10], batch_size=4, pad=vocab['<PAD>'], shuffle=False)\r\n",
        "\r\n",
        "# step 2: get a test batch from the test data generator\r\n",
        "test_inputs_batch, test_labels_batch = next(test_data_generator)\r\n",
        "\r\n",
        "# step 2-1: check the shapes of test inputs\r\n",
        "print(f'test sentences shape = {test_inputs_batch.shape}, test labels shape = {test_labels_batch.shape}')\r\n",
        "\r\n",
        "# step 3: call the prediction using training loop\r\n",
        "test_preds = training_loop.eval_model(test_inputs_batch)\r\n",
        "\r\n",
        "# step 3-1: check the shapes of test predictions and test actual labels\r\n",
        "print(f'test prediction shape = {test_preds.shape}, test target shape = {test_labels_batch.shape}')\r\n",
        "\r\n",
        "# step 3-2: print out the prediction and actual target for a sentence\r\n",
        "print(f'sentence ner prediction = {test_preds[0]}, sentence ner actual ={test_labels_batch[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEpJNznZqYZR"
      },
      "source": [
        "### Batch Prediction Evaluation\r\n",
        "The following method analyzes the performance of the given predictions(usually for a bacth) and its actual labels.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km1JysBkrWp9"
      },
      "source": [
        "# calculate the accuracy\r\n",
        "def evaluate_predictions(predictions, actuals, pad):\r\n",
        "  '''\r\n",
        "  Inputs:\r\n",
        "    predictions: prediction array with shape (batch_size, label_seq_max_length, num of classes)\r\n",
        "    acutals: actual labels for NER with shape (batch_size, label_seq_max_length)\r\n",
        "    pad: pad index in the vocabulary\r\n",
        "  Outputs:\r\n",
        "    tuple(corrects, total)\r\n",
        "  '''\r\n",
        "\r\n",
        "  # in each sequence prediction, get the max of each label.\r\n",
        "  outputs = np.argmax(predictions, axis=2)\r\n",
        "\r\n",
        "  # mask the pad tokens\r\n",
        "  mask = (actuals != pad)\r\n",
        "\r\n",
        "  # count the corrects \r\n",
        "  corrects = np.sum(outputs == actuals)\r\n",
        "\r\n",
        "  # count the total \r\n",
        "  total = np.sum(mask)\r\n",
        "\r\n",
        "  # return \r\n",
        "  return corrects, total"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXQSWQWdvsuX"
      },
      "source": [
        "### Evaluate Performance\r\n",
        "In this section, the total performance of model will be evaluated on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56jDuK85v9Lv"
      },
      "source": [
        "# evaluate performance\r\n",
        "def evaluate_performance(model, generator, pad):\r\n",
        "  '''\r\n",
        "  Inputs:\r\n",
        "    model: trained model \r\n",
        "    generator: test data generator\r\n",
        "    pad: pad token in the vocabulary\r\n",
        "\r\n",
        "  Output:\r\n",
        "    accuracy: (float) accuracy\r\n",
        "  '''\r\n",
        "  accuracy = 0\r\n",
        "  correct, total = 0, 0\r\n",
        "  i = 0\r\n",
        "\r\n",
        "  # iterate through the test generator\r\n",
        "  for batch in generator:\r\n",
        "    # get the inputs and labels\r\n",
        "    inputs, labels = batch\r\n",
        "\r\n",
        "    # get the preds using the model\r\n",
        "    preds = model(inputs)\r\n",
        "\r\n",
        "    # evaluate the peformance on the batch\r\n",
        "    b_corrects, b_total = evaluate_predictions(preds, labels, pad)\r\n",
        "    print(f'batch {i} , corrects = {b_corrects}, total = {b_total}')\r\n",
        "    i += 1\r\n",
        "\r\n",
        "    # update overall corrects and totals\r\n",
        "    correct += b_corrects\r\n",
        "    total += b_total\r\n",
        "\r\n",
        "  # calculate the accuracy\r\n",
        "  accuracy = correct/total\r\n",
        "\r\n",
        "  # return\r\n",
        "  return accuracy"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhyiK-bT08-A"
      },
      "source": [
        "### Run the Evaluation\r\n",
        "The following code run the evaluate model and get the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi367hV91MAQ"
      },
      "source": [
        "# define a test model\r\n",
        "test_model = training_loop.eval_model\r\n",
        "\r\n",
        "# define the test data generator\r\n",
        "test_data_generator = data_generator(test_s, test_l, batch_size=64, pad=vocab['<PAD>'], shuffle=False)\r\n",
        "\r\n",
        "# call the evaluate performance\r\n",
        "accuracy = evaluate_performance(test_model, test_data_generator, vocab['<PAD>'])\r\n",
        "\r\n",
        "# print out the results\r\n",
        "print(f'total accuracy = {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsibugAZ10fo"
      },
      "source": [
        "## Prediction\r\n",
        "This section shows how to use the model for a sentence to predict its Named Entities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gckLC-Lj197a"
      },
      "source": [
        "# method to predict NER of a sentence\r\n",
        "def predict(sentence, model, vocab, tags):\r\n",
        "  # sentence to sequence of id\r\n",
        "  sentence_vec = [vocab[t] if t in vocab else vocab['<UNK>'] for t in sentence.split(' ')]\r\n",
        "  \r\n",
        "  # define a batch_data variable and assign the sentence_vector as its first element.\r\n",
        "  batch_data = np.ones((1, len(sentence_vec)))\r\n",
        "  batch_data[0][:] = sentence_vec\r\n",
        "\r\n",
        "  # convert to np array\r\n",
        "  sentence = np.array(batch_data).astype(int)\r\n",
        "  \r\n",
        "  # call the trained model and get the output\r\n",
        "  output = model(sentence)\r\n",
        "\r\n",
        "  # each term has 17 values, get the max of each.\r\n",
        "  outputs = np.argmax(output, axis=2)\r\n",
        "\r\n",
        "  # find the NER tag for each token and put them in the pred list\r\n",
        "  labels = list(tags.keys())\r\n",
        "  pred = []\r\n",
        "  for i in range(len(outputs[0])):\r\n",
        "      idx = outputs[0][i] \r\n",
        "      pred_label = labels[idx]\r\n",
        "      pred.append(pred_label)\r\n",
        "  \r\n",
        "  # return list of predicted tags\r\n",
        "  return pred"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZvoyh24Enu7"
      },
      "source": [
        "### Prediction Sample\r\n",
        "In this section the NER for a sample sentence will be predicted using the tained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IFrafySEzyi"
      },
      "source": [
        "# define the sentence\r\n",
        "sentence = \"Barak Obama was the president of United States in 2010 . He loved playing on Saturday .\"\r\n",
        "\r\n",
        "# call the predict method\r\n",
        "predictions = predict(sentence, test_model, vocab, tags)\r\n",
        "\r\n",
        "for x,y in zip(sentence.split(' '), predictions):\r\n",
        "    # if the token is a Named Entity\r\n",
        "    if y != 'O':\r\n",
        "        print(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}