{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentencePiece.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBf/1mGIaNSnH6qi5poZL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaramib/NLP/blob/main/SentencePiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPr01MQUUoUL"
      },
      "source": [
        "# SentencePiece\r\n",
        "SentencePiece is a library that provides the tokenization at the sub-word levels. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6J0Zeh4qMSi"
      },
      "source": [
        "## Segmentation Algorithms\r\n",
        "It is able to be trained using 3 main popular algorithms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AC3RqpnLZP"
      },
      "source": [
        "### 1- BPE\r\n",
        "Byte Pair Encoding (BPE) is an algorihtm based on the next highest frequent pair which is proposed by Sennrich et al. (2016). This algorithm has been customized and adopted in GPT-2. \r\n",
        "\r\n",
        "**Algorithm**:\r\n",
        "1. Prepare a large enough training data (i.e. corpus)\r\n",
        "2. Define a desired subword vocabulary size\r\n",
        "3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example, the frequency of “low” is 5, then we rephrase it to “l o w </w>”: 5\r\n",
        "4. Generating a new subword according to the high frequency occurrence.\r\n",
        "5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJZKY0iloVCu"
      },
      "source": [
        "### 2- WordPiece\r\n",
        "WordPiece is a segmentation algorithm based on the *liklihood* that is propsoed by Schuster and Nakajima in 2012. \r\n",
        "\r\n",
        "**Algorithm**: \r\n",
        "1. Prepare a large enough training data (i.e. corpus)\r\n",
        "2. Define a desired subword vocabulary size\r\n",
        "3. Split word to sequence of characters\r\n",
        "4. Build a languages model based on step 3 data\r\n",
        "5. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.\r\n",
        "6. Repeating step 5until reaching subword vocabulary size which is defined in step 2 or the likelihood increase falls below a certain threshold.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdPvUyHHpSgU"
      },
      "source": [
        "### 3- Unigram Language Model\r\n",
        "Unigram language model is another algorithm for subword segmentation which is proposed by Kudo. One of the assumption is all subword occurrence are independently and subword sequence is produced by the product of subword occurrence probabilities. Like the WordPiece, the Unigram Language Model employs the concept of languages model to build subword vocabulary.\r\n",
        "\r\n",
        "**Algorithm**\r\n",
        "1. Prepare a large enough training data (i.e. corpus)\r\n",
        "2. Define a desired subword vocabulary size\r\n",
        "3. Optimize the probability of word occurrence by giving a word sequence.\r\n",
        "4. Compute the loss of each subword\r\n",
        "5. Sort the symbol by loss and keep top X % of word (e.g. X can be 80). To avoid out-of-vocabulary, character level is recommend to be included as subset of subword.\r\n",
        "6. Repeating step 3–5until reaching subword vocabulary size which is defined in step 2 or no change in step 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FveV92PWqHba"
      },
      "source": [
        "## Train Model\r\n",
        "Sentencepiece can be trained using any of above algorithms. It needs following inputs as argument to train a model:\r\n",
        "1.  Training Content: defined in *--input=train.txt*\r\n",
        "2.  Model File name: defined in *--model_prefix=m*\r\n",
        "    * *m* means the result model name will be m.model\r\n",
        "3.  Model Type: defined in *--model_type*\r\n",
        "    * default is *unigram*: Unigram Language Model\r\n",
        "    * *bpe* will train a BPE model\r\n",
        "    * *char* will be all the characters as term in vocab\r\n",
        "    * *word* will be all the words as term in the vocab\r\n",
        "4. Vocab Size: defined by *--vocab_size=100*\r\n",
        "5. User defined symbols: defined by \r\n",
        "    * *--user_defined_symbols*=<sep>,<cls>,<s>,</s>\r\n",
        "6. Change Pre_defined Symbols: can be done by adding following argument.\r\n",
        "    * --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]\r\n",
        "\r\n",
        "**Note**: BOS, EOS, PAD, UNK are defined by default. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEpFM7EfrZ2j"
      },
      "source": [
        "# install sentencepiece\r\n",
        "!pip install sentencepiece\r\n",
        "import sentencepiece as spm\r\n",
        "\r\n",
        "model_dir = \"./data/model/\"\r\n",
        "\r\n",
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\r\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\r\n",
        "spm.SentencePieceTrainer.train('--input=./data/vocab_train.txt --model_prefix=./data/mbpe --model_type=bpe --vocab_size=20000')\r\n",
        "\r\n",
        "# changind the pre-defined symbols\r\n",
        "#spm.SentencePieceTrainer.train('--input=botchan.txt --vocab_size=2000 --model_prefix=m --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[BOS] --eos_piece=[EOS]')\r\n",
        "\r\n",
        "# makes segmenter instance and loads the model file (m.model)\r\n",
        "sp = spm.SentencePieceProcessor()\r\n",
        "sp.load('./data/mbpe.model')\r\n",
        "\r\n",
        "# encode: text => id\r\n",
        "s1 = \"Four groups that advocate for immigrant rights said Thursday they will challenge Arizona 's new immigration law , which allows police to ask anyone for proof of legal U.S. residency .\"\r\n",
        "\r\n",
        "# print the encoded results, tokens and ids\r\n",
        "print(sp.encode_as_pieces(s1))\r\n",
        "print(sp.encode_as_ids(s1))\r\n",
        "\r\n",
        "# print decoded \r\n",
        "print(sp.decode_pieces(sp.encode_as_pieces(s1)))\r\n",
        "print(sp.decode_ids([1758, 1093, 32, 21, 3370, 25, 8133, 485, 26, 222, 70, 58, 959]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWQlGZcZyiAl"
      },
      "source": [
        "## Symbols:\r\n",
        "There are some pre-defined symbols such as *BOS*, *EOS*, *PAD*, *UNK*. they can be accessed using following codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NayYE8AnUkDx"
      },
      "source": [
        "# print pre-defined symbols\r\n",
        "print('bos=', sp.bos_id())\r\n",
        "print('eos=', sp.eos_id())\r\n",
        "print('unk=', sp.unk_id())\r\n",
        "print('pad=', sp.pad_id())  # disabled by default\r\n",
        "\r\n",
        "# print encoded a sentence\r\n",
        "print(sp.encode_as_ids('Hello world'))\r\n",
        "\r\n",
        "# Prepend or append bos/eos ids.\r\n",
        "print([sp.bos_id()] + sp.encode_as_ids('Hello world') + [sp.eos_id()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP3Dltrw5lW1"
      },
      "source": [
        "## Text normalization\r\n",
        "SentencePiece provides ability for text normalization using *--normaliation_rule_name* argument. Following are common type of text normalization. \r\n",
        "\r\n",
        "\r\n",
        "1. **nmt_nfkc:** NFKC normalization with some additional normalization around spaces. (default)\r\n",
        "2. **nfkc:** original: NFKC normalization.\r\n",
        "3. **nmt_nfkc_cf:** nmt_nfkc + Unicode case folding (mostly lower casing)\r\n",
        "4. **nfkc_cf:** nfkc + Unicode case folding.\r\n",
        "5. **identity:** no normalization\r\n"
      ]
    }
  ]
}